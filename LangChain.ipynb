{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are artificial intelligence (AI) systems designed to process and understand human language at incredibly high speeds. The importance of fast language models can be seen in several areas:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable applications to respond quickly to user input, making interactions feel more natural and intuitive. This is particularly important in real-time applications, such as chatbots, virtual assistants, and language translation software.\n",
      "2. **Increased Efficiency**: By processing language quickly, businesses and organizations can automate tasks, such as data entry, text summarization, and sentiment analysis, freeing up human resources for more strategic and creative work.\n",
      "3. **Enhanced Customer Service**: Fast language models can help power customer service chatbots, enabling them to respond quickly and accurately to customer inquiries, reducing wait times and improving customer satisfaction.\n",
      "4. **Real-time Language Translation**: Fast language models can facilitate real-time language translation, breaking down language barriers and enabling people who speak different languages to communicate effectively.\n",
      "5. **Sentiment Analysis and Feedback**: Fast language models can quickly analyze large volumes of text data, such as customer reviews and feedback, to provide insights on sentiment and preferences, helping businesses to identify areas for improvement.\n",
      "6. **Content Generation**: Fast language models can generate high-quality content, such as articles, social media posts, and product descriptions, at speeds that would be impossible for human writers to match.\n",
      "7. **Competitive Advantage**: Companies that leverage fast language models can gain a competitive advantage by responding quickly to changing market conditions, customer needs, and emerging trends.\n",
      "8. **Research and Development**: Fast language models can accelerate research in natural language processing (NLP) and other fields by enabling scientists to quickly test hypotheses, analyze large datasets, and explore new ideas.\n",
      "9. **Language Understanding and Learning**: Fast language models can help improve language understanding and learning by providing personalized feedback, correcting grammar and syntax errors, and offering suggestions for improvement.\n",
      "10. **Scalability**: Fast language models can handle large volumes of text data, making them ideal for applications that require processing and analyzing massive amounts of language data, such as social media monitoring, text classification, and information retrieval.\n",
      "\n",
      "To achieve fast language model performance, researchers and developers employ various techniques, including:\n",
      "\n",
      "* **Model pruning**: reducing the size and complexity of language models\n",
      "* **Knowledge distillation**: transferring knowledge from large models to smaller ones\n",
      "* **Quantization**: reducing the precision of model weights and activations\n",
      "* **Parallel processing**: using multiple computing resources to process language data in parallel\n",
      "* **Optimized algorithms**: developing algorithms that are optimized for specific hardware architectures\n",
      "\n",
      "By pushing the boundaries of language model speed and efficiency, researchers and developers can unlock new applications, improve existing ones, and drive innovation in areas like NLP, AI, and human-computer interaction.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "apikey=\"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\"\n",
    "models=['llama-3.3-70b-versatile', 'mixtral-8x7b-32768']\n",
    "parser_dateTime = DatetimeOutputParser()\n",
    "chat = ChatGroq(temperature=0, groq_api_key=apikey, model_name=models[0])\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1947, 8, 15, 0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#date time parser\n",
    "prompt_dateTime = PromptTemplate.from_template(\n",
    "    template = \"Answer the question.\\n{format_instructions}\\n{question}\",\n",
    "    input_vairables = [\"question\"],\n",
    "    partial_variables = {\"format_instructions\": parser_dateTime.get_format_instructions()},\n",
    ")\n",
    "chain = prompt_dateTime | chat \n",
    "response=chain.invoke({\"question\": \"When India got independence?\"})\n",
    "parser_dateTime.parse(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rohit Sharma',\n",
       " 'Virat Kohli',\n",
       " 'KL Rahul',\n",
       " 'Shreyas Iyer',\n",
       " 'Suryakumar Yadav',\n",
       " 'Ishan Kishan',\n",
       " 'Hardik Pandya',\n",
       " 'Ravindra Jadeja',\n",
       " 'Axar Patel',\n",
       " 'Kuldeep Yadav',\n",
       " 'Yuzvendra Chahal',\n",
       " 'Jasprit Bumrah',\n",
       " 'Mohammed Shami',\n",
       " 'Mohammed Siraj',\n",
       " 'Umran Malik',\n",
       " 'Arshdeep Singh']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "parser_list = CommaSeparatedListOutputParser()\n",
    "prompt_list = PromptTemplate.from_template(\n",
    "    template = \"Answer the question.\\n{format_instructions}\\n{question}\",\n",
    "    input_vairables = [\"question\"],\n",
    "    partial_variables = {\"format_instructions\": parser_list.get_format_instructions()},\n",
    ")\n",
    "chain = prompt_list | chat \n",
    "\n",
    "response=chain.invoke({\"question\": \"provide Current Indian Cricket Team Players\"})\n",
    "\n",
    "returned_object = parser_list.parse(response.content)\n",
    "returned_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author(name='Peter Thiel', number=4, books=['The Diversity Myth', 'Zero to One'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pydantic Output Parser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Author(BaseModel):\n",
    "    name: str = Field(description=\"The name of the author\")\n",
    "    number: int = Field(description=\"The number of books written by the author\")\n",
    "    books: list[str] = Field(description=\"The list of books they wrote\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Author)\n",
    "prompt_list = PromptTemplate.from_template(\n",
    "    template = \"Answer the question.\\n{format_instructions}\\n{question}\",\n",
    "    input_vairables = [\"question\"],\n",
    "    partial_variables = {\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt_list | chat \n",
    "\n",
    "response=chain.invoke({\"question\": \"Tell me about books written by Peter Thiel\"})\n",
    "returned_object = output_parser.parse(response.content)\n",
    "returned_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral.\n"
     ]
    }
   ],
   "source": [
    "# In Langchain, runnables are a powerful abstraction representing any callable unit of work. \n",
    "# They're used to encapsulate and manage different kinds of tasks, including LLM calls, database queries, or calls to external APIs. \n",
    "# This allows us to chain together diverse operations in a consistent and manageable way, making it easy to construct complex workflows within Langchain applications.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "sentiment_template = PromptTemplate(\n",
    "    input_variables=[\"feedback\"],\n",
    "    template=\"Determine the sentiment of this feedback and reply in one word as either 'Positive', 'Neutral', or 'Negative':\\n\\n{feedback}\"\n",
    ")\n",
    "\n",
    "user_feedback = \"The delivery was late, and the product was damaged when it arrived. However, the customer support team was very helpful in resolving the issue quickly.\"\n",
    "\n",
    "chain = sentiment_template | chat | StrOutputParser()\n",
    "feeback_sentiment = chain.invoke({\"feedback\": user_feedback})\n",
    "\n",
    "print(feeback_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Runnable Lambda\n",
    "\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "parse_template = PromptTemplate(\n",
    "    input_variables=[\"raw_feedback\"],\n",
    "    template=\"Parse and clean the following customer feedback for key information:\\n\\n{raw_feedback}\"\n",
    ")\n",
    "\n",
    "summary_template = PromptTemplate(\n",
    "    input_variables=[\"parsed_feedback\"],\n",
    "    template=\"Summarize this customer feedback in one concise sentence:\\n\\n{parsed_feedback}\"\n",
    ")\n",
    "\n",
    "sentiment_template = PromptTemplate(\n",
    "    input_variables=[\"feedback\"],\n",
    "    template=\"Determine the sentiment of this feedback and reply in one word as either 'Positive', 'Neutral', or 'Negative':\\n\\n{feedback}\"\n",
    ")\n",
    "\n",
    "format_parsed_output = RunnableLambda(lambda output: {\"parsed_feedback\": output})\n",
    "format_summary_output = RunnableLambda(lambda output: {\"feedback\": output})\n",
    "\n",
    "user_feedback = \"The delivery was late, and the product was damaged when it arrived. and, the customer support team was very bad in resolving the issue.\"\n",
    "\n",
    "chain = parse_template | chat | format_parsed_output | summary_template | chat | format_summary_output | sentiment_template| chat | StrOutputParser()\n",
    "feedback_sentiment = chain.invoke({\"raw_feedback\": user_feedback})\n",
    "\n",
    "print(feedback_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary of the user's message is: The customer had a negative experience with the customer service team, specifically with a representative they found to be unhelpful and rude, leading to strong feelings of disappointment.\n",
      "The sentiment was classifed as: Negative.\n",
      "Here's a draft apology message:\n",
      "\n",
      "\"Dear [User],\n",
      "\n",
      "We are deeply sorry to hear that your recent interaction with our customer service team did not meet your expectations. We apologize for the unhelpful and rude behavior you experienced with one of our representatives, and we understand that this has caused you significant disappointment.\n",
      "\n",
      "Please know that we take your concerns seriously and are committed to providing a better experience for our customers. We have forwarded your feedback to our Customer Service department, and they will be reviewing the incident to ensure that our representatives are providing the level of service and respect that you deserve.\n",
      "\n",
      "Once again, we apologize for the poor service you received, and we appreciate you bringing this to our attention. If there's anything else we can do to make things right, please don't hesitate to reach out to us.\n",
      "\n",
      "Thank you for your patience and understanding.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name/Company]\"\n"
     ]
    }
   ],
   "source": [
    "#Conditional routing with a custom function#\n",
    "\n",
    "\n",
    "parse_template = PromptTemplate(\n",
    "    input_variables=[\"raw_feedback\"],\n",
    "    template=\"Parse and clean the following customer feedback for key information:\\n\\n{raw_feedback}\"\n",
    ")\n",
    "\n",
    "summary_template = PromptTemplate(\n",
    "    input_variables=[\"parsed_feedback\"],\n",
    "    template=\"Summarize this customer feedback in one concise sentence:\\n\\n{parsed_feedback}\"\n",
    ")\n",
    "\n",
    "thankyou_template = PromptTemplate(\n",
    "    input_variables=[\"feedback\"],\n",
    "    template=\"Given the feedback, draft a thank you message for the user and request them to leave a positive rating on our webpage:\\n\\n{feedback}\"\n",
    ")\n",
    "\n",
    "details_template = PromptTemplate(\n",
    "    input_variables=[\"feedback\"],\n",
    "    template=\"Given the feedback, draft a message for the user and request them provide more details about their concern:\\n\\n{feedback}\"\n",
    ")\n",
    "\n",
    "apology_template = PromptTemplate(\n",
    "    input_variables=[\"feedback\"],\n",
    "    template=\"Given the feedback, draft an apology message for the user and mention that their concern has been forwarded to the relevant department:\\n\\n{feedback}\"\n",
    ")\n",
    "\n",
    "sentiment_template = PromptTemplate(\n",
    "    input_variables=[\"feedback\"],\n",
    "    template=\"Determine the sentiment of this feedback and reply in one word as either 'Positive', 'Neutral', or 'Negative':\\n\\n{feedback}\"\n",
    ")\n",
    "\n",
    "thankyou_chain = thankyou_template | chat | StrOutputParser()\n",
    "details_chain = details_template | chat | StrOutputParser()\n",
    "apology_chain = apology_template | chat | StrOutputParser()\n",
    "\n",
    "def route(info):\n",
    "    if \"postive\" in info['sentiment'].lower():\n",
    "        return thankyou_chain\n",
    "    elif \"negative\" in info['sentiment'].lower():\n",
    "        return apology_chain\n",
    "    else:\n",
    "        return details_chain\n",
    "\n",
    "#Neutral   \n",
    "# user_feedback = \"The delivery was late, and the product was damaged when it arrived. However, the customer support team was very helpful in resolving the issue quickly.\"\n",
    "# Postive\n",
    "# user_feedback = \"The customer service was fantastic. The representative was friendly, knowledgeable, and resolved my issue quickly.\"\n",
    "\n",
    "# Negative\n",
    "user_feedback = \"I was extremely disappointed with the customer service. The representative was unhelpful and rude.\"\n",
    "format_parsed_output = RunnableLambda(lambda output: {\"parsed_feedback\": output})\n",
    "summary_chain = parse_template | chat | format_parsed_output | summary_template | chat | StrOutputParser()\n",
    "sentiment_chain = sentiment_template| chat | StrOutputParser()\n",
    "summary = summary_chain.invoke({'raw_feedback' : user_feedback})\n",
    "sentiment = sentiment_chain.invoke({'feedback': summary})\n",
    "\n",
    "print(\"The summary of the user's message is:\", summary)\n",
    "print(\"The sentiment was classifed as:\", sentiment)\n",
    "\n",
    "full_chain = {\"feedback\": lambda x: x['feedback'], 'sentiment' : lambda x : x['sentiment']} | RunnableLambda(route)\n",
    "print(full_chain.invoke({'feedback': summary, 'sentiment': sentiment}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_discount\n",
      "Calculates the final price after applying a discount.\n",
      "\n",
      "Args:\n",
      "    price (float): The original price of the item.\n",
      "    discount_percentage (float): The discount percentage (e.g., 20 for 20%).\n",
      "\n",
      "Returns:\n",
      "    float: The final price after the discount is applied.\n",
      "{'price': {'title': 'Price', 'type': 'number'}, 'discount_percentage': {'title': 'Discount Percentage', 'type': 'number'}}\n",
      "85.0\n"
     ]
    }
   ],
   "source": [
    "# Tools in Langchain\n",
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def calculate_discount(price: float, discount_percentage: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the final price after applying a discount.\n",
    "\n",
    "    Args:\n",
    "        price (float): The original price of the item.\n",
    "        discount_percentage (float): The discount percentage (e.g., 20 for 20%).\n",
    "\n",
    "    Returns:\n",
    "        float: The final price after the discount is applied.\n",
    "    \"\"\"\n",
    "    if not (0 <= discount_percentage <= 100):\n",
    "        raise ValueError(\"Discount percentage must be between 0 and 100\")\n",
    "\n",
    "    discount_amount = price * (discount_percentage / 100)\n",
    "    final_price = price - discount_amount\n",
    "    return final_price\n",
    "\n",
    "print(calculate_discount.name)\n",
    "print(calculate_discount.description)\n",
    "print(calculate_discount.args)\n",
    "print(calculate_discount.invoke({\"price\":100, \"discount_percentage\": 15}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.0\n"
     ]
    }
   ],
   "source": [
    "#Tool Binding\n",
    "llm_with_tools = chat.bind_tools([calculate_discount])\n",
    "result=llm_with_tools.invoke(\"I have a T Shirt with price Rs 1000 and it have around 75% of discount what is the final price?\")\n",
    "arguments=result.tool_calls[0]['args']\n",
    "print(calculate_discount.invoke(arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
